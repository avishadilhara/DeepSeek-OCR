{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58cfc446",
   "metadata": {},
   "source": [
    "# DeepSeek-OCR with Tversky Neural Networks - Sinhala OCR\n",
    "\n",
    "This notebook trains DeepSeek-OCR enhanced with Tversky Projection layers for Sinhala OCR.\n",
    "\n",
    "**Dataset:**\n",
    "- Images: `/kaggle/input/sinhala-printed-text-dataset-400/images` (400 images)\n",
    "- Annotations: `/kaggle/input/sinhala-printed-text-dataset-400/annotations.csv`\n",
    "\n",
    "**Requirements:**\n",
    "- Kaggle GPU: T4 x2 or P100\n",
    "- RAM: 16GB+\n",
    "\n",
    "**Steps:**\n",
    "1. Environment Setup\n",
    "2. Load Tversky module\n",
    "3. Load base model\n",
    "4. Apply Tversky conversion\n",
    "5. Prepare Sinhala dataset\n",
    "6. Train with mixed precision\n",
    "7. Evaluate and save"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b22954",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6363b702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "    print(f\"  Memory: {torch.cuda.get_device_properties(i).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b60dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers>=4.37.0 accelerate>=0.25.0 bitsandbytes>=0.41.0\n",
    "!pip install -q datasets pillow tqdm\n",
    "!pip install -q ninja packaging pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef31160",
   "metadata": {},
   "source": [
    "## 2. Tversky Module Setup\n",
    "\n",
    "Option A: Clone from GitHub\n",
    "Option B: Upload tversky folder as Kaggle dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4706caca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "!git clone https://github.com/avishadilhara/DeepSeek-OCR /kaggle/working/DeepSeek-OCR\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add path based on your setup\n",
    "TVERSKY_PATHS = [\n",
    "    '/kaggle/working/DeepSeek-OCR/DeepSeek-OCR-master/DeepSeek-OCR-vllm',\n",
    "    '/kaggle/working',\n",
    "    '/kaggle/input/tversky-ocr-code'\n",
    "]\n",
    "\n",
    "for path in TVERSKY_PATHS:\n",
    "    if os.path.exists(path):\n",
    "        sys.path.insert(0, path)\n",
    "        print(f\"Added to path: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d48d4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify Tversky import\n",
    "try:\n",
    "    from tversky import (\n",
    "        TverskyProjection,\n",
    "        TverskyLMHead,\n",
    "        TverskyTrainingConfig,\n",
    "        SINHALA_OCR_TVERSKY_CONFIG,\n",
    "        create_tversky_optimizer,\n",
    "        get_tversky_regularization_loss,\n",
    "        monitor_tversky_health,\n",
    "        analyze_tversky_parameters\n",
    "    )\n",
    "    print(\"Tversky module imported successfully!\")\n",
    "except ImportError as e:\n",
    "    print(f\"Import error: {e}\")\n",
    "    print(\"Please upload the tversky folder or clone the repo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda4706c",
   "metadata": {},
   "source": [
    "## 3. Dataset Paths & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cda2b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# YOUR KAGGLE DATASET PATHS\n",
    "# ============================================\n",
    "KAGGLE_IMAGES_DIR = \"/kaggle/input/sinhala-printed-text-dataset-400/images\"\n",
    "KAGGLE_ANNOTATIONS_CSV = \"/kaggle/input/sinhala-printed-text-dataset-400/annotations.csv\"\n",
    "\n",
    "# Verify paths exist\n",
    "import os\n",
    "print(f\"Images directory exists: {os.path.exists(KAGGLE_IMAGES_DIR)}\")\n",
    "print(f\"Annotations file exists: {os.path.exists(KAGGLE_ANNOTATIONS_CSV)}\")\n",
    "\n",
    "if os.path.exists(KAGGLE_IMAGES_DIR):\n",
    "    images = os.listdir(KAGGLE_IMAGES_DIR)\n",
    "    print(f\"Number of images: {len(images)}\")\n",
    "    print(f\"Sample images: {images[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bfa33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the CSV file\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(KAGGLE_ANNOTATIONS_CSV)\n",
    "print(f\"CSV shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "print(f\"\\nFirst 10 rows:\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfac1fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "import os\n",
    "\n",
    "# ============================================\n",
    "# IMPORTANT: Update these after running the CSV inspection cell above!\n",
    "# Look at the CSV columns and set the correct column names:\n",
    "# ============================================\n",
    "# Based on your CSV inspection, set:\n",
    "#   - IMAGE_COL: column containing image filenames (e.g., 'image_filename', 'file', etc.)\n",
    "#   - TEXT_COL: column containing the Sinhala text ground truth (e.g., 'text', 'label', 'transcription')\n",
    "\n",
    "IMAGE_COL = 'image_filename'  # <-- UPDATE THIS based on your CSV!\n",
    "TEXT_COL = 'text'             # <-- UPDATE THIS based on your CSV!\n",
    "\n",
    "@dataclass\n",
    "class KaggleTrainingConfig:\n",
    "    \"\"\"Configuration optimized for Kaggle T4/P100 GPUs.\"\"\"\n",
    "    \n",
    "    # Dataset paths\n",
    "    images_dir: str = KAGGLE_IMAGES_DIR\n",
    "    annotations_csv: str = KAGGLE_ANNOTATIONS_CSV\n",
    "    \n",
    "    # CSV column names - MUST MATCH YOUR CSV STRUCTURE!\n",
    "    image_col: str = IMAGE_COL  \n",
    "    text_col: str = TEXT_COL    \n",
    "    \n",
    "    # Model\n",
    "    model_name: str = \"deepseek-ai/deepseek-vl-1.3b-chat\"\n",
    "    use_4bit: bool = True\n",
    "    use_flash_attention: bool = False  # Set False if not supported\n",
    "    \n",
    "    # Tversky\n",
    "    num_features: int = 512\n",
    "    conversion_strategy: str = 'lm_head_only'\n",
    "    feature_activation: str = 'softplus'\n",
    "    use_smooth_min: bool = True\n",
    "    smooth_min_temperature: float = 0.5\n",
    "    init_alpha: float = 0.3\n",
    "    init_beta: float = 0.7\n",
    "    init_gamma: float = 15.0\n",
    "    \n",
    "    # Training - optimized for 400 samples\n",
    "    batch_size: int = 2          # Small for T4 16GB\n",
    "    gradient_accumulation_steps: int = 4  # Effective batch = 8\n",
    "    learning_rate: float = 5e-5\n",
    "    tversky_lr_multiplier: float = 0.05\n",
    "    num_epochs: int = 20         # More epochs for small dataset\n",
    "    warmup_ratio: float = 0.1\n",
    "    max_seq_length: int = 256    # Adjust based on your text lengths\n",
    "    \n",
    "    # Mixed precision\n",
    "    fp16: bool = True\n",
    "    bf16: bool = False\n",
    "    \n",
    "    # Regularization\n",
    "    diversity_weight: float = 0.02\n",
    "    sparsity_weight: float = 0.001\n",
    "    weight_decay: float = 0.01\n",
    "    max_grad_norm: float = 1.0\n",
    "    \n",
    "    # Validation split\n",
    "    val_split: float = 0.1  # 10% for validation (40 images)\n",
    "    \n",
    "    # Paths\n",
    "    output_dir: str = '/kaggle/working/outputs'\n",
    "    \n",
    "    # Logging\n",
    "    logging_steps: int = 10\n",
    "    eval_steps: int = 50\n",
    "    save_steps: int = 100\n",
    "\n",
    "config = KaggleTrainingConfig()\n",
    "\n",
    "os.makedirs(config.output_dir, exist_ok=True)\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "print(f\"\\n⚠️  IMPORTANT: Verify these column names match your CSV!\")\n",
    "print(f\"    image_col: '{config.image_col}'\")\n",
    "print(f\"    text_col: '{config.text_col}'\")\n",
    "print()\n",
    "for k, v in vars(config).items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7827dd",
   "metadata": {},
   "source": [
    "## 4. Dataset Class for CSV Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160d9ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import torchvision.transforms as T\n",
    "\n",
    "class SinhalaOCRDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for Sinhala OCR with CSV annotations.\n",
    "    Works with DeepSeek-VL processor.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        images_dir: str,\n",
    "        annotations_csv: str,\n",
    "        tokenizer,\n",
    "        vl_processor=None,\n",
    "        image_col: str = 'image',\n",
    "        text_col: str = 'text',\n",
    "        max_length: int = 256,\n",
    "        image_size: tuple = (384, 384)\n",
    "    ):\n",
    "        self.images_dir = Path(images_dir)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.vl_processor = vl_processor\n",
    "        self.max_length = max_length\n",
    "        self.image_size = image_size\n",
    "        \n",
    "        # Load CSV\n",
    "        df = pd.read_csv(annotations_csv)\n",
    "        print(f\"CSV columns: {df.columns.tolist()}\")\n",
    "        print(f\"First row: {df.iloc[0].to_dict()}\")\n",
    "        \n",
    "        # Smart column detection for IMAGE column\n",
    "        # Priority: exact match > filename patterns > fallback\n",
    "        if image_col not in df.columns:\n",
    "            image_candidates = [\n",
    "                'image_filename', 'image_name', 'filename', 'file_name', \n",
    "                'image_path', 'path', 'img_name', 'image', 'file', 'img'\n",
    "            ]\n",
    "            for col in image_candidates:\n",
    "                if col in df.columns:\n",
    "                    image_col = col\n",
    "                    break\n",
    "            else:\n",
    "                # Look for column containing file extensions\n",
    "                for col in df.columns:\n",
    "                    sample_val = str(df[col].iloc[0])\n",
    "                    if any(ext in sample_val.lower() for ext in ['.png', '.jpg', '.jpeg', '.bmp', '.tif']):\n",
    "                        image_col = col\n",
    "                        break\n",
    "                else:\n",
    "                    image_col = df.columns[0]\n",
    "        \n",
    "        # Smart column detection for TEXT column\n",
    "        # Priority: exact match > text patterns > largest string column\n",
    "        if text_col not in df.columns:\n",
    "            text_candidates = [\n",
    "                'text', 'label', 'ground_truth', 'gt', 'transcription', \n",
    "                'annotation', 'content', 'ocr_text', 'sinhala_text', 'words'\n",
    "            ]\n",
    "            for col in text_candidates:\n",
    "                if col in df.columns:\n",
    "                    text_col = col\n",
    "                    break\n",
    "            else:\n",
    "                # Find column with longest average string length (likely the text)\n",
    "                best_col = None\n",
    "                best_len = 0\n",
    "                for col in df.columns:\n",
    "                    if col == image_col:\n",
    "                        continue\n",
    "                    try:\n",
    "                        avg_len = df[col].astype(str).str.len().mean()\n",
    "                        if avg_len > best_len:\n",
    "                            best_len = avg_len\n",
    "                            best_col = col\n",
    "                    except:\n",
    "                        pass\n",
    "                if best_col:\n",
    "                    text_col = best_col\n",
    "                else:\n",
    "                    # Last resort: pick second column\n",
    "                    text_col = df.columns[1] if len(df.columns) > 1 else df.columns[0]\n",
    "        \n",
    "        print(f\"Using columns: image='{image_col}', text='{text_col}'\")\n",
    "        print(f\"Sample image value: {df[image_col].iloc[0]}\")\n",
    "        print(f\"Sample text value: {str(df[text_col].iloc[0])[:100]}...\")\n",
    "        \n",
    "        # Create samples list\n",
    "        self.samples = []\n",
    "        missing_count = 0\n",
    "        found_paths = []\n",
    "        \n",
    "        for idx, row in df.iterrows():\n",
    "            img_name = str(row[image_col])\n",
    "            text = str(row[text_col])\n",
    "            \n",
    "            # Try to find image file with various strategies\n",
    "            img_path = None\n",
    "            \n",
    "            # Strategy 1: Direct path\n",
    "            direct_path = self.images_dir / img_name\n",
    "            if direct_path.exists():\n",
    "                img_path = direct_path\n",
    "            else:\n",
    "                # Strategy 2: Try with common extensions\n",
    "                for ext in ['', '.png', '.jpg', '.jpeg', '.PNG', '.JPG', '.JPEG']:\n",
    "                    test_path = self.images_dir / f\"{img_name}{ext}\"\n",
    "                    if test_path.exists():\n",
    "                        img_path = test_path\n",
    "                        break\n",
    "                \n",
    "                # Strategy 3: Try basename only (in case img_name has path)\n",
    "                if img_path is None:\n",
    "                    basename = Path(img_name).name\n",
    "                    for ext in ['', '.png', '.jpg', '.jpeg', '.PNG', '.JPG', '.JPEG']:\n",
    "                        test_path = self.images_dir / f\"{basename}{ext}\"\n",
    "                        if test_path.exists():\n",
    "                            img_path = test_path\n",
    "                            break\n",
    "            \n",
    "            if img_path is not None and img_path.exists():\n",
    "                self.samples.append({'image_path': img_path, 'text': text})\n",
    "                if len(found_paths) < 3:\n",
    "                    found_paths.append(str(img_path))\n",
    "            else:\n",
    "                missing_count += 1\n",
    "                if missing_count <= 3:\n",
    "                    print(f\"  Missing: {img_name} (tried: {self.images_dir / img_name})\")\n",
    "        \n",
    "        print(f\"\\nLoaded {len(self.samples)} samples\")\n",
    "        if found_paths:\n",
    "            print(f\"Sample found paths: {found_paths}\")\n",
    "        if missing_count > 0:\n",
    "            print(f\"Warning: {missing_count} images not found\")\n",
    "            # List actual files in directory for debugging\n",
    "            actual_files = list(self.images_dir.glob('*'))[:5]\n",
    "            print(f\"Actual files in {self.images_dir}: {[f.name for f in actual_files]}\")\n",
    "        \n",
    "        # Image transforms (fallback if no vl_processor)\n",
    "        self.transform = T.Compose([\n",
    "            T.Resize(image_size),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        \n",
    "        # Load image\n",
    "        image = Image.open(sample['image_path']).convert('RGB')\n",
    "        text = sample['text']\n",
    "        \n",
    "        # Use VL processor if available\n",
    "        if self.vl_processor is not None:\n",
    "            # Create conversation format for DeepSeek-VL\n",
    "            conversation = [\n",
    "                {\n",
    "                    \"role\": \"User\",\n",
    "                    \"content\": \"<image_placeholder>Extract all text from this image.\",\n",
    "                    \"images\": [str(sample['image_path'])]\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"Assistant\",\n",
    "                    \"content\": text\n",
    "                }\n",
    "            ]\n",
    "            \n",
    "            try:\n",
    "                # Process with VL processor\n",
    "                pil_images = [image]\n",
    "                prepare_inputs = self.vl_processor(\n",
    "                    conversations=conversation,\n",
    "                    images=pil_images,\n",
    "                    force_batchify=True\n",
    "                )\n",
    "                \n",
    "                return {\n",
    "                    'pixel_values': prepare_inputs.pixel_values.squeeze(0),\n",
    "                    'input_ids': prepare_inputs.input_ids.squeeze(0),\n",
    "                    'attention_mask': prepare_inputs.attention_mask.squeeze(0),\n",
    "                    'labels': prepare_inputs.input_ids.squeeze(0).clone(),\n",
    "                    'images_seq_mask': prepare_inputs.images_seq_mask.squeeze(0) if hasattr(prepare_inputs, 'images_seq_mask') else None,\n",
    "                    'images_emb_mask': prepare_inputs.images_emb_mask.squeeze(0) if hasattr(prepare_inputs, 'images_emb_mask') else None,\n",
    "                }\n",
    "            except Exception as e:\n",
    "                print(f\"VL processor error: {e}, falling back to basic processing\")\n",
    "        \n",
    "        # Fallback: basic processing\n",
    "        pixel_values = self.transform(image)\n",
    "        \n",
    "        # Tokenize text\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'pixel_values': pixel_values,\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'labels': encoding['input_ids'].squeeze(0).clone()\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec17d47",
   "metadata": {},
   "source": [
    "## 5. Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acdce63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install DeepSeek-VL package first\n",
    "!pip install -q git+https://github.com/deepseek-ai/DeepSeek-VL.git\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "def load_model(config):\n",
    "    \"\"\"Load DeepSeek-VL model with memory-efficient settings.\"\"\"\n",
    "    \n",
    "    # Import DeepSeek-VL specific modules\n",
    "    from deepseek_vl.models import VLChatProcessor, MultiModalityCausalLM\n",
    "    from deepseek_vl.utils.io import load_pil_images\n",
    "    \n",
    "    # Load tokenizer and processor\n",
    "    vl_chat_processor = VLChatProcessor.from_pretrained(config.model_name)\n",
    "    tokenizer = vl_chat_processor.tokenizer\n",
    "    \n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Quantization config\n",
    "    if config.use_4bit:\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "        )\n",
    "    else:\n",
    "        bnb_config = None\n",
    "    \n",
    "    # Load the model\n",
    "    model = MultiModalityCausalLM.from_pretrained(\n",
    "        config.model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16 if config.fp16 else torch.float32,\n",
    "    )\n",
    "    \n",
    "    # Enable gradient checkpointing on language_model component (not the wrapper)\n",
    "    if hasattr(model, 'language_model'):\n",
    "        lm = model.language_model\n",
    "        if hasattr(lm, 'gradient_checkpointing_enable'):\n",
    "            try:\n",
    "                lm.gradient_checkpointing_enable()\n",
    "                print(\"Gradient checkpointing enabled on language_model\")\n",
    "            except Exception as e:\n",
    "                print(f\"Could not enable gradient checkpointing: {e}\")\n",
    "    \n",
    "    return model, tokenizer, vl_chat_processor\n",
    "\n",
    "print(\"Loading DeepSeek-VL model...\")\n",
    "model, tokenizer, vl_processor = load_model(config)\n",
    "print(f\"Model loaded successfully!\")\n",
    "print(f\"  Model type: {type(model).__name__}\")\n",
    "\n",
    "# Get hidden size and vocab size from the language model component\n",
    "if hasattr(model, 'language_model'):\n",
    "    lm = model.language_model\n",
    "    print(f\"  Vocab size: {lm.config.vocab_size}\")\n",
    "    print(f\"  Hidden size: {lm.config.hidden_size}\")\n",
    "elif hasattr(model, 'config'):\n",
    "    print(f\"  Config: {model.config}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b378a2",
   "metadata": {},
   "source": [
    "## 6. Apply Tversky Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1052927d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from tversky import TverskyLMHead\n",
    "\n",
    "def convert_to_tversky(model, config):\n",
    "    \"\"\"Convert LM head to Tversky projection for DeepSeek-VL model.\"\"\"\n",
    "    \n",
    "    # DeepSeek-VL has a language_model component\n",
    "    if hasattr(model, 'language_model'):\n",
    "        lm = model.language_model\n",
    "        hidden_size = lm.config.hidden_size\n",
    "        vocab_size = lm.config.vocab_size\n",
    "        target_model = lm\n",
    "    else:\n",
    "        hidden_size = model.config.hidden_size\n",
    "        vocab_size = model.config.vocab_size\n",
    "        target_model = model\n",
    "    \n",
    "    print(f\"Model structure: hidden_size={hidden_size}, vocab_size={vocab_size}\")\n",
    "    \n",
    "    # Find LM head in the target model\n",
    "    lm_head_attr = None\n",
    "    old_lm_head = None\n",
    "    \n",
    "    for name in ['lm_head', 'output', 'cls', 'head']:\n",
    "        if hasattr(target_model, name):\n",
    "            old_lm_head = getattr(target_model, name)\n",
    "            lm_head_attr = name\n",
    "            print(f\"Found LM head: {name}\")\n",
    "            break\n",
    "    \n",
    "    if old_lm_head is None:\n",
    "        print(\"Could not find LM head. Model structure:\")\n",
    "        for name, module in target_model.named_children():\n",
    "            print(f\"  {name}: {type(module).__name__}\")\n",
    "        return model\n",
    "    \n",
    "    old_params = sum(p.numel() for p in old_lm_head.parameters())\n",
    "    \n",
    "    # Create Tversky head\n",
    "    new_lm_head = TverskyLMHead(\n",
    "        hidden_size=hidden_size,\n",
    "        vocab_size=vocab_size,\n",
    "        num_features=config.num_features,\n",
    "        init_from_linear=old_lm_head if isinstance(old_lm_head, nn.Linear) else None,\n",
    "        feature_activation=config.feature_activation,\n",
    "        use_smooth_min=config.use_smooth_min,\n",
    "        smooth_min_temperature=config.smooth_min_temperature,\n",
    "        init_alpha=config.init_alpha,\n",
    "        init_beta=config.init_beta,\n",
    "        init_gamma=config.init_gamma\n",
    "    )\n",
    "    \n",
    "    # Move to device\n",
    "    device = next(old_lm_head.parameters()).device\n",
    "    dtype = next(old_lm_head.parameters()).dtype\n",
    "    new_lm_head = new_lm_head.to(device=device, dtype=dtype)\n",
    "    \n",
    "    # Replace the head\n",
    "    setattr(target_model, lm_head_attr, new_lm_head)\n",
    "    \n",
    "    new_params = sum(p.numel() for p in new_lm_head.parameters())\n",
    "    \n",
    "    print(f\"\\nTversky conversion complete:\")\n",
    "    print(f\"  Original params: {old_params:,}\")\n",
    "    print(f\"  Tversky params: {new_params:,}\")\n",
    "    print(f\"  Reduction: {(1 - new_params/old_params)*100:.1f}%\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = convert_to_tversky(model, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9726feb",
   "metadata": {},
   "source": [
    "## 7. Create Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1558335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset with VL processor\n",
    "full_dataset = SinhalaOCRDataset(\n",
    "    images_dir=config.images_dir,\n",
    "    annotations_csv=config.annotations_csv,\n",
    "    tokenizer=tokenizer,\n",
    "    vl_processor=vl_processor,  # Pass the VL processor\n",
    "    image_col=config.image_col,\n",
    "    text_col=config.text_col,\n",
    "    max_length=config.max_seq_length\n",
    ")\n",
    "\n",
    "# Split into train/val\n",
    "val_size = int(len(full_dataset) * config.val_split)\n",
    "train_size = len(full_dataset) - val_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(\n",
    "    full_dataset, \n",
    "    [train_size, val_size],\n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset split:\")\n",
    "print(f\"  Train: {len(train_dataset)} samples\")\n",
    "print(f\"  Val: {len(val_dataset)} samples\")\n",
    "\n",
    "# Custom collate function to handle variable-length sequences\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Collate function that pads variable-length sequences.\"\"\"\n",
    "    # Filter out None values\n",
    "    batch = [b for b in batch if b is not None]\n",
    "    if len(batch) == 0:\n",
    "        return None\n",
    "    \n",
    "    result = {}\n",
    "    \n",
    "    for key in batch[0].keys():\n",
    "        values = [b[key] for b in batch if b[key] is not None]\n",
    "        if len(values) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Check if all tensors have the same shape\n",
    "        shapes = [v.shape for v in values]\n",
    "        \n",
    "        if all(s == shapes[0] for s in shapes):\n",
    "            # Same shape - can stack directly\n",
    "            result[key] = torch.stack(values)\n",
    "        else:\n",
    "            # Different shapes - need to pad\n",
    "            if len(shapes[0]) == 1:  # 1D tensors (sequences)\n",
    "                # Pad to max length in batch\n",
    "                max_len = max(v.shape[0] for v in values)\n",
    "                padded = []\n",
    "                for v in values:\n",
    "                    if v.shape[0] < max_len:\n",
    "                        # Pad with 0 (or pad_token_id for input_ids)\n",
    "                        pad_value = 0\n",
    "                        if key == 'labels':\n",
    "                            pad_value = -100  # Ignore index for loss\n",
    "                        padding = torch.full((max_len - v.shape[0],), pad_value, dtype=v.dtype)\n",
    "                        v = torch.cat([v, padding])\n",
    "                    padded.append(v)\n",
    "                result[key] = torch.stack(padded)\n",
    "            elif len(shapes[0]) == 2:  # 2D tensors (masks)\n",
    "                # Pad both dimensions if needed\n",
    "                max_dim0 = max(v.shape[0] for v in values)\n",
    "                max_dim1 = max(v.shape[1] for v in values)\n",
    "                padded = []\n",
    "                for v in values:\n",
    "                    if v.shape[0] < max_dim0 or v.shape[1] < max_dim1:\n",
    "                        new_v = torch.zeros(max_dim0, max_dim1, dtype=v.dtype)\n",
    "                        new_v[:v.shape[0], :v.shape[1]] = v\n",
    "                        v = new_v\n",
    "                    padded.append(v)\n",
    "                result[key] = torch.stack(padded)\n",
    "            elif len(shapes[0]) >= 3:  # 3D+ tensors (images, embeddings)\n",
    "                # For pixel_values, they should be same size from transforms\n",
    "                # If not, we need to handle differently\n",
    "                try:\n",
    "                    result[key] = torch.stack(values)\n",
    "                except RuntimeError:\n",
    "                    # Skip if can't stack (will handle in forward)\n",
    "                    print(f\"Warning: Could not stack {key}, shapes: {shapes[:3]}...\")\n",
    "                    continue\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Create loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0,  # Set to 0 for VL processor compatibility\n",
    "    pin_memory=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "print(f\"  Train batches: {len(train_loader)}\")\n",
    "print(f\"  Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07300a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test a batch\n",
    "sample_batch = next(iter(train_loader))\n",
    "print(\"Sample batch shapes:\")\n",
    "for k, v in sample_batch.items():\n",
    "    print(f\"  {k}: {v.shape}\")\n",
    "\n",
    "# Decode a sample\n",
    "sample_text = tokenizer.decode(sample_batch['input_ids'][0], skip_special_tokens=True)\n",
    "print(f\"\\nSample text: {sample_text[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1d3933",
   "metadata": {},
   "source": [
    "## 8. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5536b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tversky import create_tversky_optimizer, get_tversky_regularization_loss, monitor_tversky_health, analyze_tversky_parameters\n",
    "from torch.amp import GradScaler, autocast\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, config, train_loader, val_loader):\n",
    "        self.model = model\n",
    "        self.config = config\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.device = next(model.parameters()).device\n",
    "        self.dtype = next(model.parameters()).dtype  # Get model dtype (float16 or float32)\n",
    "        \n",
    "        # Optimizer\n",
    "        self.optimizer = create_tversky_optimizer(\n",
    "            model,\n",
    "            base_lr=config.learning_rate,\n",
    "            tversky_lr_multiplier=config.tversky_lr_multiplier,\n",
    "            weight_decay=config.weight_decay\n",
    "        )\n",
    "        \n",
    "        # Scheduler\n",
    "        total_steps = len(train_loader) * config.num_epochs // config.gradient_accumulation_steps\n",
    "        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            self.optimizer, T_max=total_steps\n",
    "        )\n",
    "        \n",
    "        # Mixed precision - updated API\n",
    "        self.scaler = GradScaler('cuda') if config.fp16 else None\n",
    "        \n",
    "        # Tracking\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.history = {'train_loss': [], 'val_loss': [], 'val_accuracy': []}\n",
    "    \n",
    "    def forward_pass(self, batch):\n",
    "        \"\"\"\n",
    "        Forward pass for DeepSeek-VL multimodal model.\n",
    "        Handles both multimodal (with images) and text-only inputs.\n",
    "        \"\"\"\n",
    "        input_ids = batch['input_ids'].to(self.device)\n",
    "        attention_mask = batch['attention_mask'].to(self.device)\n",
    "        labels = batch['labels'].to(self.device)\n",
    "        \n",
    "        # Check if we have image inputs\n",
    "        has_images = 'pixel_values' in batch and batch['pixel_values'] is not None\n",
    "        \n",
    "        if has_images:\n",
    "            # Convert pixel_values to correct dtype to match model\n",
    "            pixel_values = batch['pixel_values'].to(device=self.device, dtype=self.dtype)\n",
    "            images_seq_mask = batch.get('images_seq_mask')\n",
    "            images_emb_mask = batch.get('images_emb_mask')\n",
    "            \n",
    "            if images_seq_mask is not None:\n",
    "                images_seq_mask = images_seq_mask.to(self.device)\n",
    "            if images_emb_mask is not None:\n",
    "                images_emb_mask = images_emb_mask.to(self.device)\n",
    "            \n",
    "            # DeepSeek-VL: prepare inputs with image embeddings\n",
    "            try:\n",
    "                # Get text embeddings first and ensure same dtype\n",
    "                inputs_embeds = self.model.language_model.get_input_embeddings()(input_ids)\n",
    "                \n",
    "                # Get image embeddings from vision encoder\n",
    "                images_embeds = self.model.aligner(\n",
    "                    self.model.vision_model(pixel_values)\n",
    "                )\n",
    "                \n",
    "                # Ensure dtypes match\n",
    "                images_embeds = images_embeds.to(dtype=inputs_embeds.dtype)\n",
    "                \n",
    "                # Replace image placeholder tokens with image embeddings\n",
    "                if images_seq_mask is not None and images_emb_mask is not None:\n",
    "                    # Flatten masks if needed\n",
    "                    seq_mask = images_seq_mask.bool()\n",
    "                    emb_mask = images_emb_mask.bool()\n",
    "                    inputs_embeds[seq_mask] = images_embeds[emb_mask]\n",
    "                \n",
    "                # Forward through language model with embeddings\n",
    "                outputs = self.model.language_model(\n",
    "                    inputs_embeds=inputs_embeds,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels,\n",
    "                    return_dict=True,\n",
    "                    use_cache=False  # Disable cache for gradient checkpointing\n",
    "                )\n",
    "            except Exception as e:\n",
    "                # Fallback: try text-only forward\n",
    "                print(f\"Multimodal forward failed: {e}, using text-only...\")\n",
    "                outputs = self.model.language_model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels,\n",
    "                    return_dict=True,\n",
    "                    use_cache=False\n",
    "                )\n",
    "        else:\n",
    "            # Text-only forward pass through language model\n",
    "            outputs = self.model.language_model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels,\n",
    "                return_dict=True,\n",
    "                use_cache=False\n",
    "            )\n",
    "        \n",
    "        return outputs, labels\n",
    "        \n",
    "    def train_epoch(self, epoch):\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        pbar = tqdm(self.train_loader, desc=f\"Epoch {epoch+1}\")\n",
    "        \n",
    "        for step, batch in enumerate(pbar):\n",
    "            if batch is None:\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                if self.config.fp16:\n",
    "                    with autocast('cuda'):\n",
    "                        outputs, labels = self.forward_pass(batch)\n",
    "                        loss = outputs.loss\n",
    "                        reg_loss = get_tversky_regularization_loss(\n",
    "                            self.model,\n",
    "                            self.config.diversity_weight,\n",
    "                            self.config.sparsity_weight\n",
    "                        )\n",
    "                        loss = (loss + reg_loss) / self.config.gradient_accumulation_steps\n",
    "                    \n",
    "                    self.scaler.scale(loss).backward()\n",
    "                else:\n",
    "                    outputs, labels = self.forward_pass(batch)\n",
    "                    loss = outputs.loss\n",
    "                    reg_loss = get_tversky_regularization_loss(\n",
    "                        self.model,\n",
    "                        self.config.diversity_weight,\n",
    "                        self.config.sparsity_weight\n",
    "                    )\n",
    "                    loss = (loss + reg_loss) / self.config.gradient_accumulation_steps\n",
    "                    loss.backward()\n",
    "                \n",
    "                total_loss += loss.item() * self.config.gradient_accumulation_steps\n",
    "                num_batches += 1\n",
    "                \n",
    "            except RuntimeError as e:\n",
    "                print(f\"Error in batch {step}: {e}\")\n",
    "                self.optimizer.zero_grad()\n",
    "                continue\n",
    "            \n",
    "            if (step + 1) % self.config.gradient_accumulation_steps == 0:\n",
    "                if self.config.fp16:\n",
    "                    self.scaler.unscale_(self.optimizer)\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.max_grad_norm)\n",
    "                    self.scaler.step(self.optimizer)\n",
    "                    self.scaler.update()\n",
    "                else:\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.max_grad_norm)\n",
    "                    self.optimizer.step()\n",
    "                \n",
    "                self.scheduler.step()\n",
    "                self.optimizer.zero_grad()\n",
    "            \n",
    "            pbar.set_postfix({'loss': f'{total_loss/max(num_batches,1):.4f}'})\n",
    "        \n",
    "        return total_loss / max(num_batches, 1)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def evaluate(self):\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        total_correct = 0\n",
    "        total_tokens = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for batch in self.val_loader:\n",
    "            if batch is None:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                outputs, labels = self.forward_pass(batch)\n",
    "                \n",
    "                total_loss += outputs.loss.item()\n",
    "                num_batches += 1\n",
    "                \n",
    "                predictions = outputs.logits.argmax(dim=-1)\n",
    "                mask = labels != -100\n",
    "                total_correct += ((predictions == labels) & mask).sum().item()\n",
    "                total_tokens += mask.sum().item()\n",
    "            except Exception as e:\n",
    "                print(f\"Eval error: {e}\")\n",
    "                continue\n",
    "        \n",
    "        return {\n",
    "            'loss': total_loss / max(num_batches, 1),\n",
    "            'accuracy': total_correct / total_tokens if total_tokens > 0 else 0\n",
    "        }\n",
    "    \n",
    "    def train(self):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Starting training - {self.config.num_epochs} epochs\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "        \n",
    "        for epoch in range(self.config.num_epochs):\n",
    "            train_loss = self.train_epoch(epoch)\n",
    "            val_metrics = self.evaluate()\n",
    "            \n",
    "            self.history['train_loss'].append(train_loss)\n",
    "            self.history['val_loss'].append(val_metrics['loss'])\n",
    "            self.history['val_accuracy'].append(val_metrics['accuracy'])\n",
    "            \n",
    "            # Print summary\n",
    "            print(f\"\\nEpoch {epoch+1}/{self.config.num_epochs}:\")\n",
    "            print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "            print(f\"  Val Loss: {val_metrics['loss']:.4f}\")\n",
    "            print(f\"  Val Accuracy: {val_metrics['accuracy']*100:.2f}%\")\n",
    "            \n",
    "            # Tversky analysis\n",
    "            analysis = analyze_tversky_parameters(self.model)\n",
    "            for name, params in analysis.items():\n",
    "                print(f\"  {name}: a={params['alpha']:.3f}, b={params['beta']:.3f}, g={params['gamma']:.3f}\")\n",
    "            \n",
    "            # Check health\n",
    "            warnings = monitor_tversky_health(self.model)\n",
    "            if warnings:\n",
    "                print(f\"  Warnings: {warnings}\")\n",
    "            \n",
    "            # Save best\n",
    "            if val_metrics['loss'] < self.best_val_loss:\n",
    "                self.best_val_loss = val_metrics['loss']\n",
    "                self.save('best_model.pt')\n",
    "                print(f\"  Saved best model\")\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Training complete! Best val loss: {self.best_val_loss:.4f}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        return self.history\n",
    "    \n",
    "    def save(self, filename):\n",
    "        path = os.path.join(self.config.output_dir, filename)\n",
    "        tversky_state = {}\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if any(k in name for k in ['alpha_raw', 'beta_raw', 'gamma', 'feature_bank', 'prototype_bank']):\n",
    "                tversky_state[name] = param.data.cpu()\n",
    "        \n",
    "        torch.save({\n",
    "            'tversky_state_dict': tversky_state,\n",
    "            'config': vars(self.config),\n",
    "            'history': self.history,\n",
    "            'best_val_loss': self.best_val_loss\n",
    "        }, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c967b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer and train\n",
    "trainer = Trainer(model, config, train_loader, val_loader)\n",
    "history = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a6d9d6",
   "metadata": {},
   "source": [
    "## 9. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e88537",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history['train_loss'], label='Train')\n",
    "axes[0].plot(history['val_loss'], label='Validation')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training & Validation Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Accuracy\n",
    "axes[1].plot([acc * 100 for acc in history['val_accuracy']])\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy (%)')\n",
    "axes[1].set_title('Validation Accuracy')\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{config.output_dir}/training_curves.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3df89f5",
   "metadata": {},
   "source": [
    "## 10. Test Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe63289f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a few validation samples\n",
    "@torch.no_grad()\n",
    "def test_samples(model, tokenizer, val_dataset, num_samples=5):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    print(\"\\nSample Predictions:\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for i in range(min(num_samples, len(val_dataset))):\n",
    "        sample = val_dataset[i]\n",
    "        \n",
    "        input_ids = sample['input_ids'].unsqueeze(0).to(device)\n",
    "        \n",
    "        # Get prediction\n",
    "        outputs = model(input_ids=input_ids)\n",
    "        predictions = outputs.logits.argmax(dim=-1)\n",
    "        \n",
    "        # Decode\n",
    "        ground_truth = tokenizer.decode(sample['input_ids'], skip_special_tokens=True)\n",
    "        predicted = tokenizer.decode(predictions[0], skip_special_tokens=True)\n",
    "        \n",
    "        print(f\"\\nSample {i+1}:\")\n",
    "        print(f\"  Ground Truth: {ground_truth[:100]}\")\n",
    "        print(f\"  Predicted:    {predicted[:100]}\")\n",
    "\n",
    "test_samples(model, tokenizer, val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99475306",
   "metadata": {},
   "source": [
    "## 11. Save & Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23460c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List output files\n",
    "print(\"\\nOutput files:\")\n",
    "for f in os.listdir(config.output_dir):\n",
    "    filepath = os.path.join(config.output_dir, f)\n",
    "    size_mb = os.path.getsize(filepath) / 1e6\n",
    "    print(f\"  {f} ({size_mb:.1f} MB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182257dd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Notes\n",
    "\n",
    "### CSV Column Names\n",
    "Update `config.image_col` and `config.text_col` if your CSV has different column names.\n",
    "\n",
    "### Memory Issues\n",
    "If you get OOM errors:\n",
    "1. Reduce `batch_size` to 1\n",
    "2. Increase `gradient_accumulation_steps`\n",
    "3. Reduce `max_seq_length`\n",
    "\n",
    "### After Training\n",
    "1. Download `best_model.pt` from Output tab\n",
    "2. Use it to initialize Tversky layers for inference"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
